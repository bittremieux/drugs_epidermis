{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import tqdm\n",
    "from cuml.ensemble import RandomForestClassifier\n",
    "from mordred import Calculator, descriptors\n",
    "from rdkit import Chem\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import average_precision_score, \\\n",
    "    balanced_accuracy_score, precision_recall_curve, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV, learning_curve, \\\n",
    "    StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styling.\n",
    "plt.style.use(['seaborn-white', 'seaborn-paper'])\n",
    "plt.rc('font', family='sans-serif')\n",
    "sns.set_palette('Set1')\n",
    "sns.set_context('paper', font_scale=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds = pd.read_csv('../data/compound_smiles.csv')\n",
    "mols = compounds['SMILES (Canonical)'].apply(Chem.MolFromSmiles)\n",
    "clss = compounds['Skin'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate features using Mordred.\n",
    "mordred_calculator = Calculator(descriptors, ignore_3D=True)\n",
    "# Exclude features that contain non-numeric values.\n",
    "features = pd.DataFrame(mordred_calculator.pandas(mols)\n",
    "                        .select_dtypes(exclude='object')\n",
    "                        .astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrelationTreshold:\n",
    "    \n",
    "    def __init__(self, threshold=None):\n",
    "        self.threshold = threshold if threshold is not None else 1.0\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        corr = np.abs(np.corrcoef(X, rowvar=False))\n",
    "        self.mask = ~(np.triu(corr, k=1) > self.threshold).any(axis=1)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[:, self.mask]\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X, y)\n",
    "    \n",
    "    def get_support(self, indices=False):\n",
    "        return self.mask if not indices else np.where(self.mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 100\n",
    "test_size = 0.2\n",
    "\n",
    "variance_threshold = 0.05\n",
    "correlation_threshold = 0.95\n",
    "n_trees = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Repeatedly evaluate the model to estimate its performance.\n",
    "accuracies_train = np.zeros(n_splits, np.float32)\n",
    "accuracies_test = np.zeros(n_splits, np.float32)\n",
    "average_precisions_train = np.zeros(n_splits, np.float32)\n",
    "average_precisions_test = np.zeros(n_splits, np.float32)\n",
    "roc_aucs_train = np.zeros(n_splits, np.float32)\n",
    "roc_aucs_test = np.zeros(n_splits, np.float32)\n",
    "interval = np.linspace(0, 1, 101, dtype=np.float32)\n",
    "tprs_train = np.zeros((n_splits, 101), np.float32)\n",
    "tprs_test = np.zeros((n_splits, 101), np.float32)\n",
    "precisions_train = np.zeros((n_splits, 101), np.float32)\n",
    "precisions_test = np.zeros((n_splits, 101), np.float32)\n",
    "best_max_depth = np.zeros(n_splits, np.uint8)\n",
    "for i, (train_index, test_index) in tqdm.tqdm(\n",
    "        enumerate(\n",
    "            StratifiedShuffleSplit(\n",
    "                n_splits, test_size=test_size, random_state=42).split(\n",
    "                features.values, clss)),\n",
    "        desc='Cross-validation', total=n_splits):\n",
    "    features_train = features.values[train_index]\n",
    "    features_test = features.values[test_index]\n",
    "    clss_train, clss_test = clss[train_index], clss[test_index]\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        Pipeline([\n",
    "            ('variance_threshold', VarianceThreshold(variance_threshold)),\n",
    "            ('correlation_threshold', CorrelationTreshold(\n",
    "                correlation_threshold)),\n",
    "            ('classify', RandomForestClassifier(\n",
    "                n_estimators=n_trees, random_state=42))]),\n",
    "        param_grid={'classify__max_depth': np.arange(2, 8, 1)},\n",
    "        cv=StratifiedShuffleSplit(n_splits, test_size=test_size,\n",
    "                                  random_state=42))\n",
    "    \n",
    "    grid_search.fit(features_train, clss_train)\n",
    "    pred_train = grid_search.predict_proba(features_train)[:, 1]\n",
    "    pred_test = grid_search.predict_proba(features_test)[:, 1]\n",
    "    \n",
    "    # Compute evaluation metrics on the train and test data.\n",
    "    accuracies_train[i] = balanced_accuracy_score(\n",
    "            clss_train, np.asarray(pred_train > 0.5, np.int))\n",
    "    accuracies_test[i] = balanced_accuracy_score(\n",
    "            clss_test, np.asarray(pred_test > 0.5, np.int))\n",
    "    average_precisions_train[i] = average_precision_score(\n",
    "        clss_train, pred_train)\n",
    "    average_precisions_test[i] = average_precision_score(\n",
    "        clss_test, pred_test)\n",
    "    roc_aucs_train[i] = roc_auc_score(clss_train, pred_train)\n",
    "    roc_aucs_test[i] = roc_auc_score(clss_test, pred_test)\n",
    "    fpr_train, tpr_train, _ = roc_curve(clss_train, pred_train)\n",
    "    tprs_train[i] = np.interp(interval, fpr_train, tpr_train)\n",
    "    fpr_test, tpr_test, _ = roc_curve(clss_test, pred_test)\n",
    "    tprs_test[i] = np.interp(interval, fpr_test, tpr_test)\n",
    "    precision_train, recall_train, _ = precision_recall_curve(\n",
    "        clss_train, pred_train)\n",
    "    precisions_train[i] = np.interp(\n",
    "        interval, recall_train[::-1], precision_train[::-1])\n",
    "    precision_test, recall_test, _ = precision_recall_curve(\n",
    "        clss_test, pred_test)\n",
    "    precisions_test[i] = np.interp(\n",
    "        interval, recall_test[::-1], precision_test[::-1])\n",
    "    \n",
    "    # Save optimal hyperparameter(s).\n",
    "    best_max_depth[i] = (grid_search.best_estimator_\n",
    "                         .named_steps['classify'].max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eval.pickle', 'wb') as pf:\n",
    "    pickle.dump([accuracies_train, accuracies_test, average_precisions_train,\n",
    "                 average_precisions_test, roc_aucs_train, roc_aucs_test,\n",
    "                 interval, tprs_train, tprs_test, precisions_train,\n",
    "                 precisions_test, best_max_depth], pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eval.pickle', 'rb') as pf:\n",
    "    accuracies_train, accuracies_test, average_precisions_train, \\\n",
    "        average_precisions_test, roc_aucs_train, roc_aucs_test, \\\n",
    "        interval, tprs_train, tprs_test, precisions_train, precisions_test, \\\n",
    "        best_max_depth = pickle.load(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {'accuracy_train': np.mean(accuracies_train),\n",
    "         'accuracy_std_train': np.std(accuracies_train),\n",
    "         'average_precision_train': np.mean(average_precisions_train),\n",
    "         'average_precision_std_train': np.std(average_precisions_train),\n",
    "         'roc_auc_train': np.mean(roc_aucs_train),\n",
    "         'roc_auc_std_train': np.std(roc_aucs_train),\n",
    "         'tpr_mean_train': np.mean(tprs_train, axis=0),\n",
    "         'tpr_std_train': np.std(tprs_train, axis=0),\n",
    "         'precision_mean_train': np.mean(precisions_train, axis=0),\n",
    "         'precision_std_train': np.std(precisions_train, axis=0),\n",
    "        \n",
    "         'accuracy_test': np.mean(accuracies_test),\n",
    "         'accuracy_std_test': np.std(accuracies_test),\n",
    "         'average_precision_test': np.mean(average_precisions_test),\n",
    "         'average_precision_std_test': np.std(average_precisions_test),\n",
    "         'roc_auc_test': np.mean(roc_aucs_test),\n",
    "         'roc_auc_std_test': np.std(roc_aucs_test),\n",
    "         'tpr_mean_test': np.mean(tprs_test, axis=0),\n",
    "         'tpr_std_test': np.std(tprs_test, axis=0),\n",
    "         'precision_mean_test': np.mean(precisions_test, axis=0),\n",
    "         'precision_std_test': np.std(precisions_test, axis=0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy: {stats[\"accuracy_test\"]:.3f} ± '\n",
    "      f'{stats[\"accuracy_std_test\"]:.3f}')\n",
    "print(f'AUROC: {stats[\"roc_auc_test\"]:.3f} ± '\n",
    "      f'{stats[\"roc_auc_std_test\"]:.3f}')\n",
    "print(f'Average precision: {stats[\"average_precision_test\"]:.3f} ± '\n",
    "      f'{stats[\"average_precision_std_test\"]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = scipy.stats.mode(best_max_depth)[0][0]\n",
    "print(f'Optimal number of maximum depth (mode): {max_depth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the optimal model on the whole dataset and export.\n",
    "classifier = Pipeline([\n",
    "    ('variance_threshold', VarianceThreshold(variance_threshold)),\n",
    "    ('correlation_threshold', CorrelationTreshold(correlation_threshold)),\n",
    "    ('classify', RandomForestClassifier(\n",
    "        n_estimators=n_trees, max_depth=max_depth, random_state=42))])\n",
    "classifier.fit(features.values, clss)\n",
    "# with open('rf.pickle', 'wb') as pf:\n",
    "#     pickle.dump(classifier, pf)\n",
    "with open('rf.pickle', 'rb') as pf:\n",
    "    classifier = pickle.load(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 7\n",
    "height = width / 1.618    # Golden ratio.\n",
    "fig, axes = plt.subplots(1, 2, figsize=(width * 2, height))\n",
    "\n",
    "ax = axes[0]\n",
    "interval = np.linspace(0, 1, 101)\n",
    "tpr_train = stats['tpr_mean_train']\n",
    "tpr_train[0], tpr_train[-1] = 0, 1\n",
    "ax.plot(interval, tpr_train,\n",
    "        label=f'AUC (train) = {stats[\"roc_auc_train\"]:.3f} '\n",
    "              f'± {stats[\"roc_auc_std_train\"]:.3f}')\n",
    "ax.fill_between(interval, tpr_train - stats['tpr_std_train'],\n",
    "                tpr_train + stats['tpr_std_train'], alpha=0.2)\n",
    "tpr_test = stats['tpr_mean_test']\n",
    "tpr_test[0], tpr_test[-1] = 0, 1\n",
    "ax.plot(interval, tpr_test,\n",
    "        label=f'AUC (test) = {stats[\"roc_auc_test\"]:.3f} '\n",
    "              f'± {stats[\"roc_auc_std_test\"]:.3f}')\n",
    "ax.fill_between(interval, tpr_test - stats['tpr_std_test'],\n",
    "                tpr_test + stats['tpr_std_test'], alpha=0.2)\n",
    "        \n",
    "ax.plot([0, 1], [0, 1], c='black', ls='--')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "\n",
    "ax.legend(loc='lower right', frameon=False)\n",
    "        \n",
    "ax = axes[1]\n",
    "precision_train = stats['precision_mean_train']\n",
    "ax.plot(interval, precision_train,\n",
    "        label=f'Avg precision (train) = '\n",
    "              f'{stats[\"average_precision_train\"]:.3f} ± '\n",
    "              f'{stats[\"average_precision_std_train\"]:.3f}')\n",
    "ax.fill_between(interval, precision_train - stats['precision_std_train'],\n",
    "                precision_train + stats['precision_std_train'], alpha=0.2)\n",
    "precision_test = stats['precision_mean_test']\n",
    "ax.plot(interval, precision_test,\n",
    "        label=f'Avg precision (test) = '\n",
    "              f'{stats[\"average_precision_test\"]:.3f} ± '\n",
    "              f'{stats[\"average_precision_std_test\"]:.3f}')\n",
    "ax.fill_between(interval, precision_test - stats['precision_std_test'],\n",
    "                precision_test + stats['precision_std_test'], alpha=0.2)\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "\n",
    "ax.legend(loc='lower right', frameon=False)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.savefig('roc.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances.\n",
    "predict_proba = lambda x: classifier.predict_proba(x)[:,1]\n",
    "features_median = features.median().values.reshape((1, -1))\n",
    "explainer = shap.KernelExplainer(predict_proba, features_median,\n",
    "                                 link='logit')\n",
    "shap_values = explainer.shap_values(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 7\n",
    "width = height / 1.618    # Golden ratio.\n",
    "\n",
    "shap.summary_plot(shap_values, features, plot_type='bar',\n",
    "                  plot_size=(width, height), show=False)\n",
    "\n",
    "plt.gca().set_xlabel('Feature importance')\n",
    "\n",
    "plt.savefig('feature_importance_global.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 7\n",
    "width = height / 1.618    # Golden ratio.\n",
    "\n",
    "shap.summary_plot(shap_values, features,\n",
    "                  plot_size=(width, height), show=False)\n",
    "\n",
    "plt.gca().set_xlabel('Feature importance')\n",
    "\n",
    "plt.savefig('feature_importance_local.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 7\n",
    "height = width / 1.618    # Golden ratio.\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "\n",
    "shap.dependence_plot('MW', shap_values, features,\n",
    "                     interaction_index=None, ax=ax)\n",
    "\n",
    "fig.savefig('feature_importance_mw.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    classifier, features.values, clss, train_sizes=np.linspace(0.2, 1.0, 10),\n",
    "    cv=StratifiedShuffleSplit(n_splits, test_size=0.2, random_state=42),\n",
    "    scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 7\n",
    "height = width / 1.618    # Golden ratio.\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "ax.plot(train_sizes, train_scores_mean, 'o-', label='Training score')\n",
    "ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                train_scores_mean + train_scores_std, alpha=0.2)\n",
    "ax.plot(train_sizes, test_scores_mean, 'o-', label='Test score')\n",
    "ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                test_scores_mean + test_scores_std, alpha=0.2)\n",
    "\n",
    "ax.set_xlabel('Number of training examples')\n",
    "ax.set_ylabel('AUROC')\n",
    "\n",
    "ax.legend(loc='lower right', frameon=False)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.savefig('learning_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
