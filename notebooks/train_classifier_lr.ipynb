{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import tqdm.notebook as tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mordred import Calculator, descriptors\n",
    "from rdkit import Chem\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import average_precision_score, \\\n",
    "    balanced_accuracy_score, precision_recall_curve, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from correlation_threshold import CorrelationThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styling.\n",
    "plt.style.use(['seaborn-white', 'seaborn-paper'])\n",
    "plt.rc('font', family='sans-serif')\n",
    "sns.set_palette(['#6da7de', '#9e0059', '#dee000', '#d82222', '#5ea15d',\n",
    "                 '#943fa6', '#63c5b5', '#ff38ba', '#eb861e', '#ee266d'])\n",
    "sns.set_context('paper', font_scale=1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Molecular descriptor feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds = pd.read_csv('../data/processed/compound_smiles.csv')\n",
    "mols = compounds['smiles'].apply(Chem.MolFromSmiles)\n",
    "clss = compounds['skin'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate features using Mordred.\n",
    "mordred_calculator = Calculator(descriptors, ignore_3D=True)\n",
    "# Exclude features that contain non-numeric values.\n",
    "features = pd.DataFrame(mordred_calculator.pandas(mols, quiet=True)\n",
    "                        .select_dtypes(exclude='object')\n",
    "                        .astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 100\n",
    "test_size = 0.2\n",
    "\n",
    "variance_threshold = 0.05\n",
    "correlation_threshold = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Repeatedly evaluate the model to estimate its performance.\n",
    "accuracies_train = np.zeros(n_splits, np.float32)\n",
    "accuracies_test = np.zeros(n_splits, np.float32)\n",
    "average_precisions_train = np.zeros(n_splits, np.float32)\n",
    "average_precisions_test = np.zeros(n_splits, np.float32)\n",
    "roc_aucs_train = np.zeros(n_splits, np.float32)\n",
    "roc_aucs_test = np.zeros(n_splits, np.float32)\n",
    "interval = np.linspace(0, 1, 101, dtype=np.float32)\n",
    "tprs_train = np.zeros((n_splits, 101), np.float32)\n",
    "tprs_test = np.zeros((n_splits, 101), np.float32)\n",
    "precisions_train = np.zeros((n_splits, 101), np.float32)\n",
    "precisions_test = np.zeros((n_splits, 101), np.float32)\n",
    "hyperparams = collections.defaultdict(list)\n",
    "for i, (train_index, test_index) in tqdm.tqdm(\n",
    "        enumerate(\n",
    "            StratifiedShuffleSplit(\n",
    "                n_splits, test_size=test_size, random_state=42).split(\n",
    "                features.values, clss)),\n",
    "        desc='Cross-validation', total=n_splits):\n",
    "    features_train = features.values[train_index]\n",
    "    features_test = features.values[test_index]\n",
    "    clss_train, clss_test = clss[train_index], clss[test_index]\n",
    "    \n",
    "    hyperparam_search = RandomizedSearchCV(\n",
    "        Pipeline([\n",
    "            ('variance_threshold', VarianceThreshold(variance_threshold)),\n",
    "            ('correlation_threshold', CorrelationThreshold(\n",
    "                correlation_threshold)),\n",
    "            ('scale', StandardScaler()),\n",
    "            ('classify', LogisticRegression(\n",
    "                penalty='elasticnet', random_state=42, solver='saga'))]),\n",
    "        param_distributions={\n",
    "            'classify__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            'classify__l1_ratio': np.arange(0., 1.01, 0.05)},\n",
    "        n_iter=10,\n",
    "        cv=StratifiedShuffleSplit(n_splits, test_size=test_size,\n",
    "                                  random_state=42),\n",
    "        random_state=42)\n",
    "    \n",
    "    hyperparam_search.fit(features_train, clss_train)\n",
    "    pred_train = hyperparam_search.predict_proba(features_train)[:, 1]\n",
    "    pred_test = hyperparam_search.predict_proba(features_test)[:, 1]\n",
    "    \n",
    "    # Compute evaluation metrics on the train and test data.\n",
    "    accuracies_train[i] = balanced_accuracy_score(\n",
    "            clss_train, np.asarray(pred_train > 0.5, np.int))\n",
    "    accuracies_test[i] = balanced_accuracy_score(\n",
    "            clss_test, np.asarray(pred_test > 0.5, np.int))\n",
    "    average_precisions_train[i] = average_precision_score(\n",
    "        clss_train, pred_train)\n",
    "    average_precisions_test[i] = average_precision_score(\n",
    "        clss_test, pred_test)\n",
    "    roc_aucs_train[i] = roc_auc_score(clss_train, pred_train)\n",
    "    roc_aucs_test[i] = roc_auc_score(clss_test, pred_test)\n",
    "    fpr_train, tpr_train, _ = roc_curve(clss_train, pred_train)\n",
    "    tprs_train[i] = np.interp(interval, fpr_train, tpr_train)\n",
    "    fpr_test, tpr_test, _ = roc_curve(clss_test, pred_test)\n",
    "    tprs_test[i] = np.interp(interval, fpr_test, tpr_test)\n",
    "    precision_train, recall_train, _ = precision_recall_curve(\n",
    "        clss_train, pred_train)\n",
    "    precisions_train[i] = np.interp(\n",
    "        interval, recall_train[::-1], precision_train[::-1])\n",
    "    precision_test, recall_test, _ = precision_recall_curve(\n",
    "        clss_test, pred_test)\n",
    "    precisions_test[i] = np.interp(\n",
    "        interval, recall_test[::-1], precision_test[::-1])\n",
    "    \n",
    "    # Save optimal hyperparameter(s).\n",
    "    best_lr = hyperparam_search.best_estimator_.named_steps['classify']\n",
    "    hyperparams['C'].append(best_lr.C)\n",
    "    hyperparams['l1_ratio'].append(best_lr.l1_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {'accuracy_train': np.mean(accuracies_train),\n",
    "         'accuracy_std_train': np.std(accuracies_train),\n",
    "         'average_precision_train': np.mean(average_precisions_train),\n",
    "         'average_precision_std_train': np.std(average_precisions_train),\n",
    "         'roc_auc_train': np.mean(roc_aucs_train),\n",
    "         'roc_auc_std_train': np.std(roc_aucs_train),\n",
    "         'tpr_mean_train': np.mean(tprs_train, axis=0),\n",
    "         'tpr_std_train': np.std(tprs_train, axis=0),\n",
    "         'precision_mean_train': np.mean(precisions_train, axis=0),\n",
    "         'precision_std_train': np.std(precisions_train, axis=0),\n",
    "        \n",
    "         'accuracy_test': np.mean(accuracies_test),\n",
    "         'accuracy_std_test': np.std(accuracies_test),\n",
    "         'average_precision_test': np.mean(average_precisions_test),\n",
    "         'average_precision_std_test': np.std(average_precisions_test),\n",
    "         'roc_auc_test': np.mean(roc_aucs_test),\n",
    "         'roc_auc_std_test': np.std(roc_aucs_test),\n",
    "         'tpr_mean_test': np.mean(tprs_test, axis=0),\n",
    "         'tpr_std_test': np.std(tprs_test, axis=0),\n",
    "         'precision_mean_test': np.mean(precisions_test, axis=0),\n",
    "         'precision_std_test': np.std(precisions_test, axis=0)}\n",
    "hyperparams = {key: scipy.stats.mode(val)[0][0]\n",
    "               for key, val in hyperparams.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy: {stats[\"accuracy_test\"]:.3f} ± '\n",
    "      f'{stats[\"accuracy_std_test\"]:.3f}')\n",
    "print(f'AUROC: {stats[\"roc_auc_test\"]:.3f} ± '\n",
    "      f'{stats[\"roc_auc_std_test\"]:.3f}')\n",
    "print(f'Average precision: {stats[\"average_precision_test\"]:.3f} ± '\n",
    "      f'{stats[\"average_precision_std_test\"]:.3f}')\n",
    "print('Optimal hyperparameters:')\n",
    "for key, val in hyperparams.items():\n",
    "    print(f'    - {key} = {val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the optimal model on the entire dataset and export.\n",
    "classifier = Pipeline([\n",
    "    ('variance_threshold', VarianceThreshold(variance_threshold)),\n",
    "    ('correlation_threshold', CorrelationThreshold(correlation_threshold)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('classify', LogisticRegression(\n",
    "        penalty='elasticnet', random_state=42, solver='saga', **hyperparams))])\n",
    "classifier.fit(features.values, clss)\n",
    "_ = joblib.dump(classifier, '../data/processed/lr.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 7\n",
    "height = width / 1.618    # Golden ratio.\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "\n",
    "interval = np.linspace(0, 1, 101)\n",
    "tpr_test = stats['tpr_mean_test']\n",
    "tpr_test[0], tpr_test[-1] = 0, 1\n",
    "ax.plot(interval, tpr_test,\n",
    "        label=f'AUC = {stats[\"roc_auc_test\"]:.3f} '\n",
    "              f'± {stats[\"roc_auc_std_test\"]:.3f}')\n",
    "ax.fill_between(interval, tpr_test - stats['tpr_std_test'],\n",
    "                tpr_test + stats['tpr_std_test'], alpha=0.2)\n",
    "        \n",
    "ax.plot([0, 1], [0, 1], c='black', ls='--')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "\n",
    "ax.legend(loc='lower right', frameon=False)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.savefig('train_classifier_lr_roc.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 7\n",
    "height = width / 1.618    # Golden ratio.\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "        \n",
    "precision_test = stats['precision_mean_test']\n",
    "ax.plot(interval, precision_test,\n",
    "        label=f'Avg precision = '\n",
    "              f'{stats[\"average_precision_test\"]:.3f} ± '\n",
    "              f'{stats[\"average_precision_std_test\"]:.3f}')\n",
    "ax.fill_between(interval, precision_test - stats['precision_std_test'],\n",
    "                precision_test + stats['precision_std_test'], alpha=0.2)\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "\n",
    "ax.legend(loc='lower right', frameon=False)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.savefig('train_classifier_lr_pr.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = joblib.dump((stats, hyperparams), '../data/processed/train_classifier_lr.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
